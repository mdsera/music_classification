{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import math\n",
    "import json\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 22050\n",
    "TRACK_DURATION = 30 # measured in seconds\n",
    "SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION\n",
    "\n",
    "def save_mfcc1(dataset_path, json_path, n_mfcc=13, n_fft=2048, hop_length=512, num_segments=5): #num_segments,\n",
    "    # instead of saving track inputs, we would separate them in segments\n",
    "    \n",
    "    data = {\n",
    "        \"mapping\": [], # index of the list like ['classical', 'blues']\n",
    "        \"labels\": [], #they are the training data, like [[],[],[],...]\n",
    "        \"mfcc\": [] #they are the targets like [0,0,1,...], following the little list in the list above\n",
    "    }\n",
    "    \n",
    "    num_samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)\n",
    "    expected_num_mfcc_vectors_per_segment = math.ceil(num_samples_per_segment / hop_length)#round value up eg. 1.2->2\n",
    "    \n",
    "    #loop through all the genres in the folder\n",
    "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
    "\n",
    "        #ensure we are not at the root level\n",
    "        if dirpath is not dataset_path:\n",
    "            \n",
    "            #save the semantic level (music genre)\n",
    "            dirpath_components = dirpath.split(\"/\") # genre/blues --> ['genre', 'blues']\n",
    "            semantic_label = dirpath_components[-1] # get the value above at the right\n",
    "            data[\"mapping\"].append(semantic_label)\n",
    "            print(\"\\nProcessing {}\".format(semantic_label))\n",
    "            \n",
    "            #process file for especific genre\n",
    "            for f in filenames:\n",
    "                \n",
    "                #load the audio file\n",
    "                file_path = os.path.join(dirpath, f)\n",
    "                signal, sr = librosa.load(file_path, sr = SAMPLE_RATE)\n",
    "                \n",
    "                #process segments extracting mfcc and storing data\n",
    "                for s in range(num_segments):\n",
    "                    start_sample = num_samples_per_segment * s #starts at 0, cause s=0.\n",
    "                    finish_sample = start_sample + num_samples_per_segment #when s=0 then finish_sample=num_samples_per_segment\n",
    "\n",
    "                    mfcc = librosa.feature.mfcc(signal[start_sample:finish_sample],\n",
    "                                                sr,\n",
    "                                                n_mfcc = n_mfcc,\n",
    "                                                n_fft = n_fft,\n",
    "                                                hop_length = hop_length)\n",
    "                    mfcc = mfcc.T\n",
    "\n",
    "                    #store mfcc for segment if it has the expected length\n",
    "                    if len(mfcc) == expected_num_mfcc_vectors_per_segment:\n",
    "                        data[\"mfcc\"].append(mfcc.tolist())\n",
    "                        data[\"labels\"].append(i-1)\n",
    "                        print(\"{}, segment:{}\".format(file_path, s))\n",
    "                        \n",
    "    with open(json_path, \"w\") as fp:\n",
    "        json.dump(data, fp, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/Users/marcosera/desktop/python_projects/deep_learning/sound/data/genres_original\"\n",
    "JSON_PATH = \"data.json\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_mfcc1(DATASET_PATH, JSON_PATH, num_segments=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data.json\"\n",
    "\n",
    "def load_data(data_path):\n",
    "    \n",
    "    with open(data_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "        \n",
    "    X = np.array(data[\"mfcc\"])\n",
    "    y = np.array(data[\"labels\"])\n",
    "    \n",
    "    return X, y # X is the Inputs, y is the Targets\n",
    "\n",
    "def prepare_dataset(test_size, validation_size):\n",
    "    \n",
    "    # load the data\n",
    "    X, y = load_data(DATA_PATH)\n",
    "    \n",
    "    # create the train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n",
    "    \n",
    "    # create the train/validation split, the validation is a % of data from train data\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
    "    \n",
    "    # for CNN, TensorFlow expects a 3D array for each sample, which untill now are 2D (time frame, mfcc values)\n",
    "    # the 3rd dimention is the channel (when RGB, channel=3)\n",
    "    X_train = X_train[..., np.newaxis] # 4D --> (n_samples, time_frame, mfcc value, channel)\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "    X_validation = X_validation[..., np.newaxis]\n",
    "    \n",
    "    return X_train, X_validation, X_test, y_train, y_validation, y_test\n",
    "\n",
    "def plot_history(history):\n",
    "\n",
    "    fig, axs = plt.subplots(2)\n",
    "\n",
    "    # create accuracy sublpot\n",
    "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
    "    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
    "    axs[0].set_ylabel(\"Accuracy\")\n",
    "    axs[0].legend(loc=\"lower right\")\n",
    "    axs[0].set_title(\"Accuracy eval\")\n",
    "\n",
    "    # create error sublpot\n",
    "    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
    "    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
    "    axs[1].set_ylabel(\"Error\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].legend(loc=\"upper right\")\n",
    "    axs[1].set_title(\"Error eval\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def build_model(input_shape):\n",
    "    \n",
    "    # create model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # 1st convolutional layer\n",
    "    model.add(keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape)) \n",
    "        #(n_kernels, grid_size, activation, input_shape)\n",
    "    model.add(keras.layers.MaxPool2D((3,3), strides=(2,2), padding='same')) \n",
    "        #(grid_size, strives(vertical and horizontal), padding)\n",
    "    model.add(keras.layers.BatchNormalization()) \n",
    "        # speed up models, converging faster and more reliable\n",
    "    \n",
    "    # 2nd convolutional layer\n",
    "    model.add(keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape))\n",
    "    model.add(keras.layers.MaxPool2D((3,3), strides=(2,2), padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    # 3rd convolutional layer\n",
    "    model.add(keras.layers.Conv2D(32, (2,2), activation='relu', input_shape=input_shape)) #grid_size=(2,2)\n",
    "    model.add(keras.layers.MaxPool2D((2,2), strides=(2,2), padding='same')) #grid_size=(2,2)\n",
    "    model.add(keras.layers.BatchNormalization())    \n",
    "\n",
    "    # flatten the output and feed it into dense layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))#(n_neurons, activation)\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    \n",
    "    # output layer, softmax\n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))#(n_neurons=n_targets, activation)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def predict(model, X, y):\n",
    "    # X is 3D array (time_frame, mfcc_value, channel),\n",
    "    # but model.predict expects 4D array (batch_index, time_frame, mfcc_value, channel)\n",
    "    X = X[np.newaxis, ...]\n",
    "    prediction = model.predict(X) # the output of prediction is a 2D array, with softmax probability\n",
    "                                  # for every target. We need to extract the index which has the max value\n",
    "    \n",
    "    # extract index which has the max value\n",
    "    predicted_index = np.argmax(prediction, axis=1) # output is 1D array with the index with the highest probability\n",
    "    print(\"Expected index: {}\\n Predicted index: {}\".format(y, predicted_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # create train, validation and test sets\n",
    "    X_train, X_validation, X_test, y_train, y_validation, y_test = prepare_dataset(0.25, 0.2)\n",
    "    \n",
    "    # build CNN net\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3])#X_train.shape->(n_samples, time_frame, mfcc value, channel)\n",
    "    model = build_model(input_shape)\n",
    "    \n",
    "    # compile network\n",
    "    optimizer = keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "    model.compile(optimizer = optimizer, \n",
    "                 loss = \"sparse_categorical_crossentropy\",\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    # train the CNN\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=32, epochs=50)\n",
    "    \n",
    "    #plot graphs\n",
    "    plot_history(history)\n",
    "    \n",
    "    # evaluate the CNN on the test set\n",
    "    test_error, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(\"Accuracy on test set is: {}\".format(test_accuracy))\n",
    "    \n",
    "    # make predictions on a sample (inference)\n",
    "    X = X_test[100]\n",
    "    y = y_test[100]\n",
    "    predict(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.json\", \"r\") as fp:\n",
    "    data_ichi = json.load(fp)\n",
    "\n",
    "my_set = list(set(data_ichi['labels']))\n",
    "print(my_set)\n",
    "\n",
    "Song_styles={\n",
    "    0:\"pop\",\n",
    "    1:\"metal\",\n",
    "    2:\"disco\",\n",
    "    3:\"blues\",\n",
    "    4:\"reggae\",\n",
    "    5:\"classical\",\n",
    "    6:\"rock\",\n",
    "    7:\"hiphop\",\n",
    "    8:\"country\",\n",
    "    9:\"jazz\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK = 512\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 2\n",
    "RATE = 22500\n",
    "RECORD_SECONDS = 30\n",
    "WAVE_OUTPUT_FILENAME = \"record_from_mic.wav\"\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                frames_per_buffer=CHUNK)\n",
    "\n",
    "print(\"* recording\")\n",
    "\n",
    "frames = []\n",
    "\n",
    "for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "    data = stream.read(CHUNK)\n",
    "    frames.append(data)\n",
    "\n",
    "print(\"* done recording\")\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()\n",
    "\n",
    "wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "wf.setnchannels(CHANNELS)\n",
    "wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "wf.setframerate(RATE)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WAVE_OUTPUT_FILENAME = \"record_from_mic.wav\"\n",
    "SAMPLE_RATE = 22050\n",
    "TRACK_DURATION = 30\n",
    "SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION\n",
    "\n",
    "def save_mfcc1(dataset_path, json_path, n_mfcc=13, n_fft=2048, hop_length=512, num_segments=5):\n",
    "    \n",
    "    data = {\"mfcc\": []}\n",
    "    \n",
    "    num_samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)\n",
    "    expected_num_mfcc_vectors_per_segment = math.ceil(num_samples_per_segment / hop_length)\n",
    "                \n",
    "    #load the audio file\n",
    "    file_path = os.path.join(dataset_path, WAVE_OUTPUT_FILENAME)\n",
    "    signal, sr = librosa.load(file_path, sr = SAMPLE_RATE)\n",
    "\n",
    "    #process segments extracting mfcc and storing data\n",
    "    for s in range(num_segments):\n",
    "        start_sample = num_samples_per_segment * s #starts at 0, cause s=0.\n",
    "        finish_sample = start_sample + num_samples_per_segment #when s=0 then finish_sample=num_samples_per_segment\n",
    "\n",
    "        mfcc = librosa.feature.mfcc(signal[start_sample:finish_sample],\n",
    "                                    sr,\n",
    "                                    n_mfcc = n_mfcc,\n",
    "                                    n_fft = n_fft,\n",
    "                                    hop_length = hop_length)\n",
    "        mfcc = mfcc.T\n",
    "\n",
    "        #store mfcc for segment if it has the expected length\n",
    "        if len(mfcc) == expected_num_mfcc_vectors_per_segment:\n",
    "            data[\"mfcc\"].append(mfcc.tolist())\n",
    "#             print(\"{}, segment:{}\".format(file_path, s))\n",
    "                        \n",
    "    with open(json_path, \"w\") as fp:\n",
    "        json.dump(data, fp, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/Users/marcosera/desktop/python_projects/deep_learning/sound/\"\n",
    "JSON_PATH = \"record_from_mic.json\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_mfcc1(DATASET_PATH, JSON_PATH, num_segments=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"record_from_mic.json\", \"r\") as fp:\n",
    "    data_prev = json.load(fp)\n",
    "X = np.array(data_prev[\"mfcc\"])\n",
    "print(\"Inicial X shape\",X.shape)\n",
    "\n",
    "print(\"X_test shape\", X_test[100].shape)\n",
    "print(\"X shape\", X.shape)\n",
    "X = X[..., np.newaxis]\n",
    "print(\"X new axis\", X.shape)\n",
    "a_correct = X[0,...]\n",
    "print(\"a_correct shape\", a_correct.shape)\n",
    "\n",
    "os.remove(\"/Users/marcosera/desktop/python_projects/deep_learning/sound/record_from_mic.json\")\n",
    "os.remove(\"/Users/marcosera/desktop/python_projects/deep_learning/sound/record_from_mic.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X)\n",
    "predicted_index = np.argmax(prediction, axis=1)\n",
    "print(\"Predicted index: {}\".format(predicted_index))\n",
    "\n",
    "df=pd.DataFrame({'Number': predicted_index})\n",
    "list_df = df['Number'].tolist()\n",
    "count_dict = dict(Counter(list_df).items())\n",
    "print(count_dict)\n",
    "k = Counter(count_dict)\n",
    "n=1\n",
    "if len(count_dict)>2:\n",
    "    high = k.most_common(3)\n",
    "\n",
    "    for i in high:\n",
    "        print(\"{} Most probable: {}\".format(n,Song_styles[i[0]]))\n",
    "        n+=1\n",
    "else:\n",
    "    high = k.most_common(len(count_dict))\n",
    "    for i in high:\n",
    "        print(\"{} Most probable: {}\".format(n,Song_styles[i[0]]))\n",
    "        n+=1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
